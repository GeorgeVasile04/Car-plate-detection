{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2962cff",
   "metadata": {},
   "source": [
    "# Enhanced License Plate Detection with Bidirectional FPN\n",
    "\n",
    "This notebook implements an improved license plate detection model using a bidirectional Feature Pyramid Network (BiFPN) architecture. The goal is to achieve a mean IoU of 0.6 or higher, specifically by improving detection of small license plates which form the majority of the dataset.\n",
    "\n",
    "### Key improvements over the previous model:\n",
    "\n",
    "1. **Enhanced BiFPN Architecture** with bidirectional feature flow and attention mechanisms\n",
    "2. **Size-Aware Loss Functions** that place greater emphasis on width/height accuracy\n",
    "3. **Small Plate Focused Augmentation** with adaptive strategies based on plate size\n",
    "4. **Improved Training Strategies** including cyclic learning rates and focused early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e816d17",
   "metadata": {},
   "source": [
    "## Step 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96896a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sys\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Add the project root directory to the path\n",
    "project_root = str(Path(os.getcwd()).parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import custom modules from the license plate detection package\n",
    "from license_plate_detection.data.enhanced_augmentation import (\n",
    "    augment_data_with_small_plate_focus, \n",
    "    create_tf_data_augmenter\n",
    ")\n",
    "from license_plate_detection.models.enhanced_losses import (\n",
    "    ultra_plate_detection_loss,\n",
    "    enhanced_ciou_loss,\n",
    "    enhanced_focal_loss,\n",
    "    enhanced_iou_metric\n",
    ")\n",
    "from license_plate_detection.models.enhanced_fpn import (\n",
    "    create_bifpn_license_plate_detector,\n",
    "    enable_gradient_checkpointing\n",
    ")\n",
    "from license_plate_detection.train.optimization import (\n",
    "    create_lr_scheduler,\n",
    "    setup_gpu_memory_growth,\n",
    "    limit_gpu_memory,\n",
    "    clean_memory,\n",
    "    optimize_memory_usage\n",
    ")\n",
    "from license_plate_detection.train.trainer  import create_efficient_data_pipeline\n",
    "from license_plate_detection.evaluation.evaluator import evaluate_model_comprehensive\n",
    "from license_plate_detection.evaluation.error_analysis import analyze_error_patterns\n",
    "from license_plate_detection.utils.visualization import (\n",
    "    visualize_bounding_boxes, \n",
    "    visualize_augmentation,\n",
    "    plot_predictions\n",
    ")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Set image size - using a higher resolution for better small plate detection\n",
    "IMAGE_SIZE = (320, 320)  # Previous model used (224, 224)\n",
    "\n",
    "# Configure TensorFlow to handle GPU memory more efficiently\n",
    "setup_gpu_memory_growth()\n",
    "\n",
    "# Check for available GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(f\"Found device: {device.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac919c9b",
   "metadata": {},
   "source": [
    "## Step 2: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d3a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with increased image resolution\n",
    "print(\"Loading and preprocessing dataset...\")\n",
    "df, X, y = load_dataset(\n",
    "    dataset_path=os.path.join(project_root, 'Dataset'),\n",
    "    image_size=IMAGE_SIZE,\n",
    "    normalize=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Display dataset statistics\n",
    "print(f\"Dataset loaded: {len(X)} images with shape {X[0].shape}\")\n",
    "\n",
    "# Visualize a few examples\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(6, len(X))):\n",
    "    visualize_bounding_boxes(X[i], [y[i]], ax=axes[i])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4093c2b",
   "metadata": {},
   "source": [
    "### Analyze distribution of plate sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate plate sizes (area) as percentage of image\n",
    "plate_areas = np.array([box[2] * box[3] for box in y])  # width * height\n",
    "\n",
    "# Define thresholds for small, medium, and large plates\n",
    "small_threshold = 0.05  # 5% of image area\n",
    "large_threshold = 0.1   # 10% of image area\n",
    "\n",
    "# Count plates in each category\n",
    "small_count = np.sum(plate_areas < small_threshold)\n",
    "large_count = np.sum(plate_areas >= large_threshold)\n",
    "medium_count = len(plate_areas) - small_count - large_count\n",
    "\n",
    "print(f\"Plate size distribution:\")\n",
    "print(f\"  Small plates (<{small_threshold*100}% of image): {small_count} ({small_count/len(y)*100:.1f}%)\")\n",
    "print(f\"  Medium plates: {medium_count} ({medium_count/len(y)*100:.1f}%)\")\n",
    "print(f\"  Large plates (>={large_threshold*100}% of image): {large_count} ({large_count/len(y)*100:.1f}%)\")\n",
    "\n",
    "# Plot histogram of plate sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(plate_areas * 100, bins=30, alpha=0.7, color='blue')\n",
    "plt.axvline(small_threshold * 100, color='r', linestyle='--', alpha=0.5, label=f'Small plate threshold ({small_threshold*100}%)')\n",
    "plt.axvline(large_threshold * 100, color='g', linestyle='--', alpha=0.5, label=f'Large plate threshold ({large_threshold*100}%)')\n",
    "plt.xlabel('Plate Area (% of image)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of License Plate Sizes')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3751b2",
   "metadata": {},
   "source": [
    "### Enhanced Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the enhanced augmentation with focus on small plates\n",
    "print(\"Generating augmented dataset with special focus on small plates...\")\n",
    "X_aug, y_aug = augment_data_with_small_plate_focus(\n",
    "    X, y, \n",
    "    augmentation_factor=5  # Increased from 4 in the original\n",
    ")\n",
    "\n",
    "# Visualize some augmented samples\n",
    "visualize_augmentation(X, y, X_aug, y_aug, num_samples=4)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = split_dataset(X_aug, y_aug, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8a4f7",
   "metadata": {},
   "source": [
    "## Step 3: Enhanced BiFPN Architecture Implementation\n",
    "\n",
    "We implement a bidirectional Feature Pyramid Network (BiFPN) with the following improvements:\n",
    "\n",
    "1. **Bidirectional feature flow** - Information flows both top-down and bottom-up\n",
    "2. **Channel and spatial attention** - To focus on important features\n",
    "3. **Scale-aware detection heads** - Specialized processing for different feature scales\n",
    "4. **Weighted feature fusion** - Better integration of multi-scale features\n",
    "5. **Increased input resolution** - 320x320 instead of 224x224 for better small plate detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the enhanced BiFPN model\n",
    "print(\"Creating enhanced BiFPN license plate detector...\")\n",
    "bifpn_model = create_bifpn_license_plate_detector(input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "# Display model summary\n",
    "bifpn_model.summary()\n",
    "\n",
    "# Print model size information\n",
    "trainable_count = np.sum([keras.backend.count_params(w) for w in bifpn_model.trainable_weights])\n",
    "non_trainable_count = np.sum([keras.backend.count_params(w) for w in bifpn_model.non_trainable_weights])\n",
    "print(f'Total parameters: {trainable_count + non_trainable_count:,}')\n",
    "print(f'Trainable parameters: {trainable_count:,}')\n",
    "print(f'Non-trainable parameters: {non_trainable_count:,}')\n",
    "\n",
    "# Apply memory optimizations\n",
    "print(\"Applying gradient checkpointing to reduce memory usage...\")\n",
    "bifpn_model = enable_gradient_checkpointing(bifpn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a3d89",
   "metadata": {},
   "source": [
    "## Step 5: GPU Memory Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6031d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable mixed precision training to reduce memory usage and increase speed on GPU\n",
    "try:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision policy set to\", policy.name)\n",
    "    print(\"Compute dtype:\", policy.compute_dtype)\n",
    "    print(\"Variable dtype:\", policy.variable_dtype)\n",
    "except Exception as e:\n",
    "    print(f\"Could not enable mixed precision: {e} - using default precision\")\n",
    "\n",
    "# Clean up memory before starting training\n",
    "clean_memory()\n",
    "\n",
    "# Optimize training data formats\n",
    "X_train, X_val, y_train, y_val = optimize_memory_usage(X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"Memory optimization complete. Training should now be more stable on GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba28d802",
   "metadata": {},
   "source": [
    "## Step 6: Advanced Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7759aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with our custom loss function\n",
    "bifpn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=ultra_plate_detection_loss,  # Our enhanced loss function\n",
    "    metrics=[\n",
    "        enhanced_iou_metric,  # Primary metric for monitoring\n",
    "        enhanced_ciou_loss,   # For monitoring size accuracy\n",
    "        enhanced_focal_loss   # For monitoring hard example performance\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set up callbacks with optimized hyperparameters\n",
    "callbacks = [\n",
    "    # Cosine annealing learning rate scheduler with warm restarts\n",
    "    create_lr_scheduler(\n",
    "        scheduler_type='cosine_restart',  \n",
    "        initial_learning_rate=0.001,\n",
    "        first_decay_steps=15,            # First cycle length\n",
    "        t_mul=1.5,                       # Cycle length growth factor\n",
    "        m_mul=0.75,                      # LR decay factor for each restart\n",
    "        alpha=1e-5                       # Minimum LR value\n",
    "    ),\n",
    "    \n",
    "    # More permissive early stopping focused on IoU improvement\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_enhanced_iou_metric',\n",
    "        patience=20,                     # Longer patience for better convergence\n",
    "        restore_best_weights=True,\n",
    "        mode='max'\n",
    "    ),\n",
    "    \n",
    "    # Model checkpoint to save best model\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'enhanced_bifpn_license_plate_detector.h5',\n",
    "        monitor='val_enhanced_iou_metric',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # TensorBoard logging\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='./logs/bifpn_model',\n",
    "        histogram_freq=1,\n",
    "        update_freq='epoch'\n",
    "    ),\n",
    "    \n",
    "    # Custom callback to reduce learning rate on plateau if needed\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_enhanced_iou_metric',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-6,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create optimized data pipelines with improved batch size\n",
    "BATCH_SIZE = 16  # Reduced from 24 to accommodate larger model and higher resolution\n",
    "\n",
    "print(\"Creating efficient data pipeline...\")\n",
    "train_dataset, val_dataset = create_efficient_data_pipeline(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Create on-the-fly augmentation function\n",
    "tf_augmenter = create_tf_data_augmenter(cutout_prob=0.3, mixup_prob=0.2)\n",
    "\n",
    "# Apply on-the-fly augmentation to the training dataset\n",
    "augmented_train_dataset = train_dataset.map(\n",
    "    tf_augmenter,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5e202",
   "metadata": {},
   "source": [
    "## Step 7: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb08aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training timer\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Starting Enhanced BiFPN model training...\")\n",
    "print(\"Training with the following improvements:\")\n",
    "print(\"- Bidirectional feature fusion through BiFPN\")\n",
    "print(\"- Enhanced attention mechanisms (channel, spatial, and dual)\")\n",
    "print(\"- Scale-aware detection heads for different feature levels\")\n",
    "print(\"- Ultra plate detection loss focusing on size accuracy\")\n",
    "print(\"- Small plate focused augmentation\")\n",
    "print(\"- Higher input resolution (320x320)\")\n",
    "print(\"- Cosine annealing LR schedule with warm restarts\")\n",
    "\n",
    "# Train the model\n",
    "history = bifpn_model.fit(\n",
    "    augmented_train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=70,  # Increased from 50 for better convergence\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "hours = int(training_time // 3600)\n",
    "minutes = int((training_time % 3600) // 60)\n",
    "seconds = int(training_time % 60)\n",
    "print(f\"Training completed in {hours}h {minutes}m {seconds}s\")\n",
    "\n",
    "# Reference to trained model\n",
    "bifpn_trained_model = bifpn_model\n",
    "\n",
    "# Save the final model\n",
    "bifpn_model.save('bifpn_license_plate_detector_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf3df65",
   "metadata": {},
   "source": [
    "## Step 8: Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddbc458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot IoU metric\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['enhanced_iou_metric'])\n",
    "plt.plot(history.history['val_enhanced_iou_metric'])\n",
    "plt.title('IoU Metric')\n",
    "plt.ylabel('IoU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot focal loss component\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['enhanced_focal_loss'])\n",
    "plt.plot(history.history['val_enhanced_focal_loss'])\n",
    "plt.title('Focal Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "if history.history:\n",
    "    epochs_trained = len(history.history['loss'])\n",
    "    print(f\"Total epochs trained: {epochs_trained}\")\n",
    "    \n",
    "    print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    \n",
    "    train_iou = history.history['enhanced_iou_metric'][-1]\n",
    "    val_iou = history.history['val_enhanced_iou_metric'][-1]\n",
    "    print(f\"Final training IoU: {train_iou:.4f}\")\n",
    "    print(f\"Final validation IoU: {val_iou:.4f}\")\n",
    "    \n",
    "    # Get best IoU\n",
    "    best_epoch = np.argmax(history.history['val_enhanced_iou_metric'])\n",
    "    best_val_iou = history.history['val_enhanced_iou_metric'][best_epoch]\n",
    "    print(f\"Best validation IoU: {best_val_iou:.4f} (epoch {best_epoch+1})\")\n",
    "else:\n",
    "    print(\"No training history found. Training may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f7c71",
   "metadata": {},
   "source": [
    "## Step 9: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ca454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from the trained model\n",
    "print(\"Generating predictions from the trained model...\")\n",
    "bifpn_predictions = bifpn_trained_model.predict(X_val)\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "print(\"Running comprehensive evaluation...\")\n",
    "bifpn_results = evaluate_model_comprehensive(\n",
    "    bifpn_trained_model,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    bifpn_predictions\n",
    ")\n",
    "\n",
    "print(f\"BiFPN model mean IoU: {bifpn_results['mean_iou']:.4f}\")\n",
    "print(f\"BiFPN model mAP@0.5: {bifpn_results['map50']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc5ce3",
   "metadata": {},
   "source": [
    "## Step 10: Error Analysis by Plate Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform detailed error analysis by plate size categories\n",
    "bifpn_error_analysis = analyze_error_patterns(\n",
    "    model=bifpn_trained_model,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    y_pred=bifpn_predictions,\n",
    "    plate_sizes=[box[2] * box[3] for box in y_val]\n",
    ")\n",
    "\n",
    "# Display detailed error metrics\n",
    "print(\"\\nDetailed Error Analysis:\")\n",
    "print(f\"- Mean IoU: {bifpn_error_analysis['mean_iou']:.4f}\")\n",
    "print(f\"- Position error: {bifpn_error_analysis['position_error']:.4f}\")\n",
    "print(f\"- Size error: {bifpn_error_analysis['size_error']:.4f}\")\n",
    "print(f\"- Aspect ratio error: {bifpn_error_analysis['aspect_ratio_error']:.4f}\")\n",
    "\n",
    "# Display error metrics by plate size category\n",
    "print(\"\\nPerformance by Plate Size:\")\n",
    "for size_cat, metrics in bifpn_error_analysis['by_size_category'].items():\n",
    "    print(f\"- {size_cat.capitalize()} plates ({metrics['count']} samples):\")\n",
    "    print(f\"  - IoU: {metrics['iou']:.4f}\")\n",
    "    print(f\"  - Position error: {metrics['position_error']:.4f}\")\n",
    "    print(f\"  - Size error: {metrics['size_error']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8afa6",
   "metadata": {},
   "source": [
    "## Step 11: Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cebb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on validation set\n",
    "# Select specific examples from each plate size category\n",
    "plate_areas = np.array([box[2] * box[3] for box in y_val])\n",
    "small_indices = np.where(plate_areas < 0.05)[0]\n",
    "medium_indices = np.where((plate_areas >= 0.05) & (plate_areas < 0.1))[0]\n",
    "large_indices = np.where(plate_areas >= 0.1)[0]\n",
    "\n",
    "# Sample from each category\n",
    "num_samples = 3\n",
    "small_samples = np.random.choice(small_indices, size=min(num_samples, len(small_indices)), replace=False)\n",
    "medium_samples = np.random.choice(medium_indices, size=min(num_samples, len(medium_indices)), replace=False)\n",
    "large_samples = np.random.choice(large_indices, size=min(num_samples, len(large_indices)), replace=False)\n",
    "\n",
    "# Combine samples\n",
    "sample_indices = np.concatenate([small_samples, medium_samples, large_samples])\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot predictions vs ground truth\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    if i < len(axes):\n",
    "        # Get ground truth and prediction\n",
    "        true_box = y_val[idx]\n",
    "        pred_box = bifpn_predictions[idx]\n",
    "        \n",
    "        # Calculate IoU for this sample\n",
    "        iou = plot_predictions(X_val[idx], true_box, pred_box, ax=axes[i])\n",
    "        \n",
    "        # Add category label\n",
    "        area = true_box[2] * true_box[3] * 100\n",
    "        if area < 5:\n",
    "            category = \"Small\"\n",
    "        elif area < 10:\n",
    "            category = \"Medium\"\n",
    "        else:\n",
    "            category = \"Large\"\n",
    "            \n",
    "        axes[i].set_title(f\"{category} plate (Area: {area:.1f}%, IoU: {iou:.4f})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c80ee",
   "metadata": {},
   "source": [
    "## Step 12: Compare with Previous FPN Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous FPN evaluation results (if available)\n",
    "try:\n",
    "    # You could load saved metrics from a file\n",
    "    # or use hardcoded values from previous runs\n",
    "    prev_metrics = {\n",
    "        'mean_iou': 0.2514,  # Previous mean IoU\n",
    "        'small_iou': 0.2227,  # Previous IoU for small plates\n",
    "        'medium_iou': 0.3247,  # Previous IoU for medium plates\n",
    "        'large_iou': 0.4755,  # Previous IoU for large plates\n",
    "        'size_error': 1.6132  # Previous size error\n",
    "    }\n",
    "    \n",
    "    # Extract current metrics for comparison\n",
    "    current_metrics = {\n",
    "        'mean_iou': bifpn_error_analysis['mean_iou'],\n",
    "        'small_iou': bifpn_error_analysis['by_size_category']['small']['iou'],\n",
    "        'medium_iou': bifpn_error_analysis['by_size_category']['medium']['iou'],\n",
    "        'large_iou': bifpn_error_analysis['by_size_category']['large']['iou'],\n",
    "        'size_error': bifpn_error_analysis['size_error']\n",
    "    }\n",
    "    \n",
    "    # Calculate improvement percentages\n",
    "    improvements = {\n",
    "        key: (current_metrics[key] - prev_metrics[key]) / prev_metrics[key] * 100 \n",
    "        if key != 'size_error' else \n",
    "        (prev_metrics[key] - current_metrics[key]) / prev_metrics[key] * 100\n",
    "        for key in prev_metrics\n",
    "    }\n",
    "    \n",
    "    # Display comparison\n",
    "    print(\"Comparison with Previous FPN Model:\")\n",
    "    print(f\"Mean IoU: {prev_metrics['mean_iou']:.4f} → {current_metrics['mean_iou']:.4f} ({improvements['mean_iou']:+.1f}%)\")\n",
    "    print(f\"Small plate IoU: {prev_metrics['small_iou']:.4f} → {current_metrics['small_iou']:.4f} ({improvements['small_iou']:+.1f}%)\")\n",
    "    print(f\"Medium plate IoU: {prev_metrics['medium_iou']:.4f} → {current_metrics['medium_iou']:.4f} ({improvements['medium_iou']:+.1f}%)\")\n",
    "    print(f\"Large plate IoU: {prev_metrics['large_iou']:.4f} → {current_metrics['large_iou']:.4f} ({improvements['large_iou']:+.1f}%)\")\n",
    "    print(f\"Size error: {prev_metrics['size_error']:.4f} → {current_metrics['size_error']:.4f} ({improvements['size_error']:+.1f}%)\")\n",
    "    \n",
    "    # Plot comparison bar chart\n",
    "    categories = ['Mean IoU', 'Small Plate IoU', 'Medium Plate IoU', 'Large Plate IoU']\n",
    "    prev_values = [prev_metrics['mean_iou'], prev_metrics['small_iou'], prev_metrics['medium_iou'], prev_metrics['large_iou']]\n",
    "    curr_values = [current_metrics['mean_iou'], current_metrics['small_iou'], current_metrics['medium_iou'], current_metrics['large_iou']]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(x - width/2, prev_values, width, label='Previous FPN')\n",
    "    plt.bar(x + width/2, curr_values, width, label='Enhanced BiFPN')\n",
    "    \n",
    "    plt.ylabel('IoU')\n",
    "    plt.title('Performance Comparison')\n",
    "    plt.xticks(x, categories)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(prev_values):\n",
    "        plt.text(i - width/2, v + 0.02, f\"{v:.3f}\", ha='center')\n",
    "    \n",
    "    for i, v in enumerate(curr_values):\n",
    "        plt.text(i + width/2, v + 0.02, f\"{v:.3f}\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not create comparison with previous model: {e}\")\n",
    "    print(\"Current model results:\")\n",
    "    print(f\"- Mean IoU: {bifpn_error_analysis['mean_iou']:.4f}\")\n",
    "    for size_cat, metrics in bifpn_error_analysis['by_size_category'].items():\n",
    "        print(f\"- {size_cat.capitalize()} plate IoU: {metrics['iou']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f34ff",
   "metadata": {},
   "source": [
    "## Step 13: Conclusion and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3edb818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final conclusion\n",
    "print(\"\\nEnhanced BiFPN License Plate Detection Results Summary:\")\n",
    "print(f\"Overall mean IoU: {bifpn_error_analysis['mean_iou']:.4f}\")\n",
    "\n",
    "# Check if target IoU of 0.6 was achieved\n",
    "target_iou = 0.6\n",
    "if bifpn_error_analysis['mean_iou'] >= target_iou:\n",
    "    print(f\"✅ Successfully achieved target IoU of {target_iou}!\")\n",
    "    improvement = (bifpn_error_analysis['mean_iou'] - target_iou) / target_iou * 100\n",
    "    print(f\"   Performance is {improvement:.1f}% above the target.\")\n",
    "else:\n",
    "    print(f\"❌ Target IoU of {target_iou} not yet achieved.\")\n",
    "    shortfall = (target_iou - bifpn_error_analysis['mean_iou']) / target_iou * 100\n",
    "    print(f\"   Performance is {shortfall:.1f}% below the target.\")\n",
    "    \n",
    "    # Suggestions for further improvement\n",
    "    print(\"\\nSuggestions for further improvement:\")\n",
    "    print(\"1. Increase model capacity with additional layers or filters\")\n",
    "    print(\"2. Further tune the loss function weights for specific error patterns\")\n",
    "    print(\"3. Increase image resolution to 384×384 or higher\")\n",
    "    print(\"4. Apply advanced learning techniques like knowledge distillation\")\n",
    "    print(\"5. Implement test-time augmentation for inference\")\n",
    "\n",
    "print(\"\\nKey improvements in this implementation:\")\n",
    "print(\"1. Enhanced BiFPN architecture with bidirectional feature fusion\")\n",
    "print(\"2. Specialized loss function focusing on size estimation accuracy\")\n",
    "print(\"3. Targeted augmentation strategies for small plates\")\n",
    "print(\"4. Increased resolution (320×320) for better small object detection\")\n",
    "print(\"5. Advanced learning rate scheduling with cosine annealing\")\n",
    "\n",
    "print(\"\\nNext steps for deployment:\")\n",
    "print(\"1. Model export to TFLite or ONNX format for production\")\n",
    "print(\"2. Implementation of a post-processing pipeline for multi-plate scenarios\")\n",
    "print(\"3. Integration with license plate recognition (LPR) system\")\n",
    "print(\"4. Performance optimization for real-time inference\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
